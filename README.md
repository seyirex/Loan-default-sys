# Loan Default Prediction - MLOps System

A production-ready ML inference system for loan default prediction demonstrating MLOps best practices.

[![Python 3.11](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/downloads/release/python-3110/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.129.0-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://www.docker.com/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-Ready-blue.svg)](https://kubernetes.io/)

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Development Workflow](#development-workflow)
- [API Documentation](#api-documentation)
- [API Examples](#api-examples)
- [Kubernetes Deployment](#kubernetes-deployment)
- [Monitoring](#monitoring)
- [Testing](#testing)
- [Project Structure](#project-structure)
- [Environment Variables](#environment-variables)
- [Troubleshooting](#troubleshooting)

## ğŸ¯ Overview

This system provides a complete MLOps pipeline for loan default prediction, including:

- **Training Pipeline**: Production training script with MLflow tracking and model registry
- **Inference API**: FastAPI REST API with authentication and rate limiting
- **Batch Processing**: Async batch predictions using Celery
- **Drift Detection**: PSI-based feature drift monitoring
- **Metrics**: Prometheus metrics for observability
- **Containerization**: Docker and Kubernetes ready

**Model Performance:**
- Accuracy: 89%
- Recall: 77%
- F1 Score: 0.31
- ROC-AUC: 0.87

## ğŸ—ï¸ Architecture

![Architecture Diagram](images/diagram.png)

### Request Flow Diagram

**Real-time Prediction Request (Docker Compose):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST http://localhost:8005/api/v1/predict
     â”‚ Header: X-API-Key: your-secret-api-key
     â”‚ Body: {employed: 1, bank_balance: 10000, annual_salary: 50000}
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Container (port 8005)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Authentication Middleware                 â”‚ â”‚
â”‚  â”‚    â€¢ Verify X-API-Key header                 â”‚ â”‚
â”‚  â”‚    â€¢ Return 403 if invalid                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 2. Rate Limiting (SlowAPI)                   â”‚ â”‚
â”‚  â”‚    â€¢ Check: 100 requests/minute per IP       â”‚ â”‚
â”‚  â”‚    â€¢ Return 429 if limit exceeded            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 3. Request Validation (Pydantic)             â”‚ â”‚
â”‚  â”‚    â€¢ Type checking (int, float)              â”‚ â”‚
â”‚  â”‚    â€¢ Range validation (salary > 0)           â”‚ â”‚
â”‚  â”‚    â€¢ Return 422 if invalid                   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 4. Feature Engineering (preprocessing.py)    â”‚ â”‚
â”‚  â”‚    â€¢ Calculate Saving_Rate                   â”‚ â”‚
â”‚  â”‚    â€¢ Formula: Bank_Balance / Annual_Salary   â”‚ â”‚
â”‚  â”‚    â€¢ Create feature vector [4 features]      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 5. Model Inference (ModelService)            â”‚ â”‚
â”‚  â”‚    â€¢ Retrieve cached model from app_state    â”‚ â”‚
â”‚  â”‚    â€¢ Apply StandardScaler (fit during train) â”‚ â”‚
â”‚  â”‚    â€¢ XGBoost.predict_proba() [~10ms]         â”‚ â”‚
â”‚  â”‚    â€¢ Get class (0/1) + probability           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                                  â”‚
â”‚                 â”‚ Model loaded from MLflow         â”‚
â”‚                 â”‚ (once at startup)                â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚    MLflow Model Registry Access              â”‚ â”‚
â”‚  â”‚    â€¢ Model: file:///app/mlflow               â”‚ â”‚
â”‚  â”‚    â€¢ Stage: Production                       â”‚ â”‚
â”‚  â”‚    â€¢ Artifacts: model.pkl + scaler.pkl       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 6. Drift Detection (DriftDetector)           â”‚ â”‚
â”‚  â”‚    â€¢ Sample: 10% of predictions              â”‚ â”‚
â”‚  â”‚    â€¢ Calculate PSI vs reference (1000 pred)  â”‚ â”‚
â”‚  â”‚    â€¢ Alert if PSI > 0.15 for any feature     â”‚ â”‚
â”‚  â”‚    â€¢ Async/non-blocking                      â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 7. Metrics Recording (Prometheus)            â”‚ â”‚
â”‚  â”‚    â€¢ loan_predictions_total++                â”‚ â”‚
â”‚  â”‚    â€¢ loan_prediction_duration_seconds        â”‚ â”‚
â”‚  â”‚    â€¢ loan_prediction_result_total{class=0}   â”‚ â”‚
â”‚  â”‚    â€¢ loan_model_drift_psi{feature=...}       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 8. Response Formatting                       â”‚ â”‚
â”‚  â”‚    â€¢ Map: 0 â†’ "no_default", 1 â†’ "default"    â”‚ â”‚
â”‚  â”‚    â€¢ Risk: Low (<0.3), Medium, High (>0.7)   â”‚ â”‚
â”‚  â”‚    â€¢ Return JSON with model metadata         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Client receives response      â”‚
     â”‚  Status: 200 OK                â”‚
     â”‚  {                             â”‚
     â”‚    "success": true,            â”‚
     â”‚    "data": {                   â”‚
     â”‚      "prediction": 0,          â”‚
     â”‚      "probability": 0.0823,    â”‚
     â”‚      "default_risk": "Low",    â”‚
     â”‚      "model_version": "1"      â”‚
     â”‚    }                           â”‚
     â”‚  }                             â”‚
     â”‚  Total Duration: ~15-25ms      â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Batch Prediction Request (Celery + Redis):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚ POST http://localhost:8005/api/v1/predict/batch
     â”‚ Body: {predictions: [{...}, {...}, ...]}  (up to 1000)
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Container (port 8005)              â”‚
â”‚  1. Validate request (max 1000 predictions)        â”‚
â”‚  2. Create Celery task via batch_service.py        â”‚
â”‚  3. Return job_id immediately (202 Accepted)       â”‚
â”‚     Duration: ~5ms                                 â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Task queued via Celery
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis Container (port 6389)                   â”‚
â”‚  â€¢ Broker: Stores task in "celery" queue           â”‚
â”‚  â€¢ Task data: job_id, predictions array, metadata  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Celery worker polls queue every 1s
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Celery Worker Container (2 concurrent workers)  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 1. Receive task from Redis broker            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 2. Load model from MLflow                    â”‚ â”‚
â”‚  â”‚    â€¢ Same model as API (shared volume)       â”‚ â”‚
â”‚  â”‚    â€¢ Cached after first load                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 3. Batch Processing                          â”‚ â”‚
â”‚  â”‚    â€¢ Process in chunks of 100                â”‚ â”‚
â”‚  â”‚    â€¢ Feature engineering for each row        â”‚ â”‚
â”‚  â”‚    â€¢ Model inference (vectorized)            â”‚ â”‚
â”‚  â”‚    â€¢ ~100ms per 100 predictions              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 4. Drift Detection (5% sample rate)          â”‚ â”‚
â”‚  â”‚    â€¢ Lower sampling for batch vs online      â”‚ â”‚
â”‚  â”‚    â€¢ Same PSI calculation                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 5. Store results in Redis                    â”‚ â”‚
â”‚  â”‚    â€¢ Key: celery-task-meta-{job_id}          â”‚ â”‚
â”‚  â”‚    â€¢ Value: {status, result, traceback}      â”‚ â”‚
â”‚  â”‚    â€¢ TTL: 24 hours                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ 6. Task Complete                             â”‚ â”‚
â”‚  â”‚    â€¢ Update status: PENDING â†’ SUCCESS        â”‚ â”‚
â”‚  â”‚    â€¢ Log completion with duration            â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â”‚ Results stored in Redis
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Redis Container (Result Backend)              â”‚
â”‚  â€¢ Results accessible via job_id                   â”‚
â”‚  â€¢ Client polls: GET /api/v1/predict/batch/{id}    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜
                                                   â”‚
              Client polling every 2-5s             â”‚
                                                   â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FastAPI returns result           â”‚
                    â”‚  {                                â”‚
                    â”‚    "job_id": "abc-123",           â”‚
                    â”‚    "status": "SUCCESS",           â”‚
                    â”‚    "total_predictions": 500,      â”‚
                    â”‚    "results": [                   â”‚
                    â”‚      {"prediction": 0, "prob": 0.1},â”‚
                    â”‚      ...                          â”‚
                    â”‚    ],                             â”‚
                    â”‚    "processing_time": "2.3s"      â”‚
                    â”‚  }                                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Model Loading at Startup:**
```
Docker Container Start
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI lifespan event (main.py)  â”‚
â”‚  Triggered once at startup          â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ModelService.load_model()          â”‚
â”‚  1. Connect to MLflow tracking URI  â”‚
â”‚     â€¢ file:///app/mlflow (local)    â”‚
â”‚  2. Query registry for model        â”‚
â”‚     â€¢ Name: "loan-default-xgboost"  â”‚
â”‚     â€¢ Stage: "Production"           â”‚
â”‚  3. Download artifacts               â”‚
â”‚     â€¢ model.pkl                     â”‚
â”‚     â€¢ scaler.pkl                    â”‚
â”‚  4. Load into memory (~30s)         â”‚
â”‚  5. Store in app_state              â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Readiness probe succeeds           â”‚
â”‚  GET /readyz returns 200            â”‚
â”‚  Container ready for traffic        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Monitoring & Observability:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Prometheus Scraper                   â”‚
â”‚  â€¢ Scrapes: http://localhost:8005/metrics      â”‚
â”‚  â€¢ Interval: 15s                               â”‚
â”‚  â€¢ Stores time-series data                     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Custom Metrics Exposed                        â”‚
â”‚  â€¢ loan_predictions_total (counter)            â”‚
â”‚  â€¢ loan_prediction_duration_seconds (histogram)â”‚
â”‚  â€¢ loan_prediction_result_total (counter)      â”‚
â”‚  â€¢ loan_model_drift_psi (gauge)                â”‚
â”‚  â€¢ loan_model_drift_detected (gauge)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Flow Characteristics:**
- **Latency**: Real-time predictions: 15-25ms, Batch: ~10ms per prediction
- **Throughput**: 4 Uvicorn workers = ~400 req/sec per container
- **State**: Stateless API, state in MLflow (models) and Redis (jobs)
- **Scalability**: Horizontal scaling via Docker Compose replicas or K8s HPA
- **Resilience**: Health checks, retry logic (Celery), graceful degradation

## âœ¨ Features

### Core Functionality
- âœ… Real-time loan default prediction API
- âœ… Batch prediction processing with job tracking
- âœ… Automatic feature engineering (Saving Rate calculation)
- âœ… SMOTE-based class balancing
- âœ… Production model auto-promotion based on metrics

### MLOps Best Practices
- âœ… MLflow experiment tracking and model registry
- âœ… Population Stability Index (PSI) drift detection
- âœ… Prometheus metrics and monitoring
- âœ… API key authentication
- âœ… Rate limiting (100 req/min)
- âœ… Structured logging with Loguru
- âœ… Health checks for Kubernetes probes

### Infrastructure
- âœ… Docker containerization (Python 3.11-slim images ~300MB)
- âœ… Docker Compose orchestration (API, MLflow, Redis, Celery)
- âœ… Hot-reloading for local development
- âœ… Kubernetes manifests with HPA (3-10 pod autoscaling)
- âœ… Resource limits and requests
- âœ… Persistent volumes for MLflow artifacts and database
- âœ… Health checks and readiness probes

## ğŸ”§ Prerequisites

- **Docker** (20.10+) and Docker Compose (2.0+)
- **Python** 3.11 (only if running outside Docker)
- **Kubernetes** cluster (optional, for K8s deployment)
- **kubectl** (optional, for K8s deployment)

## ğŸš€ Quick Start

### 1. Clone and Setup

```bash
# Clone the repository
cd loan-default-sys

# Create environment file
cp .env.example .env

# Edit .env and set your API key
# Change: API_KEY=your-secret-api-key-here-change-in-production
```

### 2. Build Docker Images

```bash
docker-compose build
```

### 3. Train the Model

```bash
docker-compose run --rm api python training/train.py
```

This will:
- Load and preprocess data
- Train XGBoost with SMOTE
- Log metrics to MLflow
- Save model to registry
- Auto-promote to Production stage if metrics meet criteria

**View training results:**
- Open MLflow UI at http://localhost:5001
- Navigate to "loan-default-prediction" experiment
- View metrics, parameters, and model artifacts

**Transfer trained model to another server:**

The trained model and MLflow artifacts are stored in `./mlflow` directory. To deploy on another server without retraining:

```bash
# On training server - package MLflow artifacts
tar -czf mlflow-artifacts.tar.gz mlflow/

# Transfer to deployment server
scp mlflow-artifacts.tar.gz user@deployment-server:/path/to/loan-default-sys/

# On deployment server - extract artifacts
tar -xzf mlflow-artifacts.tar.gz

# Start services (skip training step)
docker-compose up
```

The API will automatically load the Production model from the `./mlflow` directory at startup.

### 4. Start Services

```bash
docker-compose up
```

Services will be available at:
- **API**: http://localhost:8005
- **API Docs**: http://localhost:8005/docs
- **Health Check**: http://localhost:8005/healthz
- **Metrics**: http://localhost:8005/metrics
- **MLflow UI**: http://localhost:5001
- **Redis**: localhost:6389

### 5. Make a Prediction

```bash
curl -X POST "http://localhost:8005/api/v1/predict" \
  -H "X-API-Key: your-secret-api-key-here-change-in-production" \
  -H "Content-Type: application/json" \
  -d '{
    "employed": 1,
    "bank_balance": 10000.0,
    "annual_salary": 50000.0
  }'
```

**Response:**
```json
{
  "success": true,
  "data": {
    "prediction": 0,
    "probability": 0.0823,
    "default_risk": "Low",
    "model_version": "1",
    "features_used": {
      "employed": 1,
      "bank_balance": 10000.0,
      "annual_salary": 50000.0
    }
  },
  "message": "Prediction completed successfully"
}
```

**ğŸ“š For more examples, see [EXAMPLES.md](EXAMPLES.md)**

## ğŸ”„ Development Workflow

### Docker Compose Services

The system runs 4 containerized services:

| Service | Port | Description |
|---------|------|-------------|
| **api** | 8005 | FastAPI inference service with auto-reload |
| **mlflow** | 5001 | MLflow tracking UI and model registry |
| **redis** | 6389 | Message broker for Celery tasks |
| **celery-worker** | - | Background worker for batch predictions |

```bash
# Start all services
docker-compose up

# Start in detached mode
docker-compose up -d

# View logs
docker-compose logs -f api          # API logs only
docker-compose logs -f              # All services

# Stop services
docker-compose down

# Rebuild after dependency changes
docker-compose up --build
```

### Local Development with Hot-Reloading

The docker-compose setup includes hot-reloading for rapid development:

```bash
# Start services with auto-reload
docker-compose up

# Edit files in src/, tests/, or training/ - changes apply immediately
# API automatically reloads when you save Python files
```

**Mounted Volumes:**
- `./src` â†’ `/app/src` - API code changes reload automatically
- `./tests` â†’ `/app/tests` - Test changes reflect immediately
- `./training` â†’ `/app/training` - Training script updates available instantly

### Running Tests

```bash
# Run all tests with coverage (HTML report auto-generated)
docker-compose run --rm api pytest

# Run specific test file
docker-compose run --rm api pytest tests/test_api/test_health.py -v

# Run tests by marker
docker-compose run --rm api pytest -m unit
docker-compose run --rm api pytest -m integration
docker-compose run --rm api pytest -m slow

# View HTML coverage report (opens in browser)
open htmlcov/index.html
```

### Code Quality

```bash
# Format code with Black (88 character line length)
docker-compose run --rm api black src/ tests/

# Lint with Ruff (auto-fix issues)
docker-compose run --rm api ruff check src/ tests/ --fix

# Type check with mypy
docker-compose run --rm api mypy src/

# Run all quality checks
docker-compose run --rm api black src/ tests/ && \
docker-compose run --rm api ruff check src/ tests/ --fix && \
docker-compose run --rm api mypy src/
```

### View Logs

```bash
# API logs
docker-compose logs -f api

# Celery worker logs
docker-compose logs -f celery-worker

# All logs
docker-compose logs -f
```

## ğŸ“š API Documentation

### Authentication

All protected endpoints require an API key in the header:

```bash
X-API-Key: your-secret-api-key
```

### Endpoints

#### `GET /healthz` - Liveness Probe
Returns 200 if service is alive.

#### `GET /readyz` - Readiness Probe
Returns 200 if service is ready (model loaded).

#### `POST /api/v1/predict` - Real-time Prediction
Make a single prediction.

**Request Body:**
```json
{
  "employed": 1,
  "bank_balance": 10000.0,
  "annual_salary": 50000.0
}
```

**Rate Limit:** 100 requests/minute

#### `POST /api/v1/predict/batch` - Submit Batch Job
Submit batch predictions for async processing.

**Request Body:**
```json
{
  "predictions": [
    {"employed": 1, "bank_balance": 10000, "annual_salary": 50000},
    {"employed": 0, "bank_balance": 5000, "annual_salary": 30000}
  ]
}
```

**Response:**
```json
{
  "job_id": "uuid-string",
  "status": "PENDING",
  "message": "Batch job submitted successfully",
  "total_predictions": 2
}
```

#### `GET /api/v1/predict/batch/{job_id}` - Check Batch Status
Check status of batch job.

#### `GET /api/v1/model/info` - Model Information
Get information about deployed model.

#### `GET /metrics` - Prometheus Metrics
Prometheus-formatted metrics (no auth required).

---

## ğŸ“š API Examples

For comprehensive examples including curl commands and full request/response pairs, see **[EXAMPLES.md](EXAMPLES.md)**.

The examples file includes:
- âœ… Single predictions (low, medium, high risk scenarios)
- âœ… Batch prediction workflow
- âœ… Model information queries
- âœ… Health check examples
- âœ… Error cases and validation examples
- âœ… Input/output field specifications

## â˜¸ï¸ Kubernetes Deployment

### Prerequisites

- Kubernetes cluster (Minikube, GKE, EKS, AKS, etc.)
- kubectl configured
- Container image pushed to registry

### Build and Push Image

```bash
# Build image
docker build -t your-registry/loan-default-api:latest .

# Push to registry
docker push your-registry/loan-default-api:latest

# Update k8s/deployment-api.yaml and k8s/deployment-celery-worker.yaml
# Change: image: loan-default-api:latest
# To: image: your-registry/loan-default-api:latest
```

### Deploy to Kubernetes

```bash
# Create namespace and resources
kubectl apply -f k8s/namespace.yaml
kubectl apply -f k8s/configmap.yaml
kubectl apply -f k8s/secret.yaml
kubectl apply -f k8s/pvc.yaml

# Deploy services
kubectl apply -f k8s/deployment-redis.yaml
kubectl apply -f k8s/service-redis.yaml
kubectl apply -f k8s/deployment-api.yaml
kubectl apply -f k8s/service-api.yaml
kubectl apply -f k8s/deployment-celery-worker.yaml
kubectl apply -f k8s/hpa.yaml
```

### Verify Deployment

```bash
# Check pods
kubectl get pods -n loan-default-prediction

# Check services
kubectl get svc -n loan-default-prediction

# Check HPA
kubectl get hpa -n loan-default-prediction

# View logs
kubectl logs -f deployment/loan-api -n loan-default-prediction
```

### Access API

```bash
# Port forward to access API
kubectl port-forward svc/loan-api 8005:80 -n loan-default-prediction

# API now available at http://localhost:8005
```

## ğŸ“Š Monitoring

### Prometheus Metrics

The `/metrics` endpoint exposes:

**Custom Metrics:**
- `loan_predictions_total` - Total predictions counter
- `loan_prediction_duration_seconds` - Prediction latency histogram
- `loan_prediction_result_total` - Prediction results (default/no_default)
- `loan_model_drift_psi` - PSI score per feature
- `loan_model_drift_detected` - Drift detection binary indicator

**Standard Metrics:**
- HTTP request counts, latencies, and status codes
- Process metrics (CPU, memory, etc.)

### Drift Detection

The system automatically monitors feature drift using PSI:

- **Reference Data**: First 1000 predictions
- **Window Size**: 100 recent predictions
- **Threshold**: PSI > 0.15 triggers alert

Drift alerts are logged and exposed via Prometheus metrics.

## ğŸ§ª Testing

Test coverage: **>70%**

The project uses pytest with configured markers and automatic coverage reporting.

```bash
# Run all tests (generates both terminal and HTML coverage reports)
docker-compose run --rm api pytest

# Run with specific markers
docker-compose run --rm api pytest -m unit         # Unit tests only
docker-compose run --rm api pytest -m integration  # Integration tests only
docker-compose run --rm api pytest -m slow         # Slow running tests only

# Run specific test directories
docker-compose run --rm api pytest tests/test_api/ -v
docker-compose run --rm api pytest tests/test_services/ -v

# View detailed coverage report
open htmlcov/index.html
```

**Test Structure:**
```
tests/
â”œâ”€â”€ conftest.py              # Shared fixtures
â”œâ”€â”€ test_api/
â”‚   â”œâ”€â”€ test_health.py       # Health endpoint tests
â”‚   â”œâ”€â”€ test_predict.py      # Prediction endpoint tests
â”‚   â””â”€â”€ test_model.py        # Model endpoint tests
â””â”€â”€ test_services/
    â”œâ”€â”€ test_model_service.py      # Model loading tests
    â”œâ”€â”€ test_batch_service.py      # Celery batch tests
    â”œâ”€â”€ test_drift_detector.py     # Drift detection tests
    â”œâ”€â”€ test_metrics_service.py    # Prometheus metrics tests
    â””â”€â”€ test_training_service.py   # Training pipeline tests
```

## ğŸ“ Project Structure

```
loan-default-sys/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/v1/
â”‚   â”‚   â”œâ”€â”€ health.py         # Health check endpoints
â”‚   â”‚   â”œâ”€â”€ predict.py        # Prediction endpoints
â”‚   â”‚   â””â”€â”€ model.py          # Model info endpoint
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ model_service.py    # Model loading and inference
â”‚   â”‚   â”œâ”€â”€ training_service.py # Model training orchestration
â”‚   â”‚   â”œâ”€â”€ drift_detector.py   # PSI drift detection
â”‚   â”‚   â”œâ”€â”€ metrics_service.py  # Prometheus metrics
â”‚   â”‚   â””â”€â”€ batch_service.py    # Celery tasks
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ health.py         # Health schemas
â”‚   â”‚   â””â”€â”€ prediction.py     # Prediction schemas
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ preprocessing.py  # Utility functions
â”‚   â”œâ”€â”€ config.py             # Configuration
â”‚   â”œâ”€â”€ logging_config.py     # Logging setup
â”‚   â””â”€â”€ main.py               # FastAPI app
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ Default_Fin.csv       # Dataset
â”‚   â””â”€â”€ loan-default-prediction.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api/             # API tests
â”‚   â””â”€â”€ test_services/        # Service tests
â”œâ”€â”€ k8s/                      # Kubernetes manifests
â”œâ”€â”€ mlflow/                   # MLflow artifacts
â”œâ”€â”€ Dockerfile                # Container image
â”œâ”€â”€ docker-compose.yml        # Local development
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pytest.ini                # Pytest configuration
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ EXAMPLES.md               # API request/response examples
â””â”€â”€ DESIGN.md                 # Design document
```

## ğŸ” Environment Variables

See [.env.example](.env.example) for all configuration options.

**Key Variables:**

| Variable | Description | Default |
|----------|-------------|---------|
| `API_KEY` | API authentication key | `your-secret-api-key` |
| `MODEL_STAGE` | MLflow model stage to load | `Production` |
| `RATE_LIMIT_PER_MINUTE` | Rate limit for `/predict` | `100` |
| `DRIFT_PSI_THRESHOLD` | PSI threshold for drift alert | `0.15` |
| `DRIFT_SAMPLING_RATE` | Probability of drift check per request | `0.1` (10%) |
| `LOG_LEVEL` | Logging level | `INFO` |
| `REDIS_HOST` | Redis hostname | `redis` |
| `REDIS_PORT` | Redis port | `6379` |

**Optional MLflow Auth (set in docker-compose.yml):**
- `MLFLOW_TRACKING_USERNAME` - MLflow UI username
- `MLFLOW_TRACKING_PASSWORD` - MLflow UI password
- `MLFLOW_FLASK_SERVER_SECRET_KEY` - Flask secret key for MLflow

## ğŸ› Troubleshooting

### Model not loading

**Issue**: `Model service not initialized` error

**Solution**:
1. Ensure you've trained the model: `docker-compose run --rm api python training/train.py`
2. Check MLflow directory: `ls -la mlflow/`
3. Check logs: `docker-compose logs api`

### Permission denied errors (on server deployments)

**Issue**: `Permission denied: '/app/mlruns/...'` or `Failed to load model` with permission errors

**Solution**:
```bash
# Run the permission fix script
./fix-mlflow-permissions.sh

# Or manually fix permissions
sudo chown -R 1000:1000 mlflow/
sudo chmod -R 755 mlflow/

# Restart services
docker-compose restart
```

**Note**: This is common when transferring MLflow artifacts between servers or after copying from Docker volumes.

### Database locked errors

**Issue**: `sqlite3.OperationalError: database is locked` in MLflow logs

**Solution**:
SQLite doesn't handle concurrent access well. The docker-compose.yml is configured to enable WAL mode automatically, but if you're seeing this error:

```bash
# Manually enable WAL mode
sqlite3 mlflow/mlflow.db "PRAGMA journal_mode=WAL;"

# Or run the fix script which includes this
./fix-mlflow-permissions.sh

# Restart services
docker-compose restart
```

**Production recommendation**: For high-concurrency production deployments, migrate MLflow to PostgreSQL backend (see k8s/deployment-postgres.yaml).

### Celery worker not processing jobs

**Issue**: Batch jobs stuck in PENDING

**Solution**:
1. Check Redis is running: `docker-compose ps redis`
2. Verify Redis health: `redis-cli -h localhost -p 6389 ping` (should return PONG)
3. Check Celery logs: `docker-compose logs -f celery-worker`
4. Restart services: `docker-compose restart celery-worker redis`

### API key errors

**Issue**: 403 Forbidden errors

**Solution**:
1. Check .env file has correct API key
2. Ensure header is `X-API-Key` (case-sensitive)
3. Restart services after changing .env: `docker-compose restart api`

### Out of memory errors

**Issue**: Container crashes with OOM

**Solution**:
1. Increase Docker Desktop memory allocation (8GB+ recommended)
2. Reduce batch size in batch predictions
3. Adjust resource limits in k8s manifests

## ğŸ“„ License

This project is created for educational and demonstration purposes.

## ğŸ‘¥ Author

Created as an MLOps take-home exercise demonstrating production ML system design.

---

